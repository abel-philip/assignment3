# -*- coding: utf-8 -*-
"""Deploy_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mgk5K_z-vlgc2L84tV1ci1oWiJJUkTPz
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

# Commented out IPython magic to ensure Python compatibility.
# %cd MyDrive/ass3

# Commented out IPython magic to ensure Python compatibility.
# %cd /gdrive/MyDrive/exported_model/

# Commented out IPython magic to ensure Python compatibility.
# %ls

import sys
# Confirm that we're using Python 3
assert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'

!pip --version

# TensorFlow and tf.keras
print("Installing dependencies for Colab environment")
!pip3 install -Uq grpcio==1.26.0

import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import os
import subprocess

print('TensorFlow version: {}'.format(tf.__version__))

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

#MODEL_DIR = "/gdrive/MyDrive/exported_model/9/"

# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo
# You would instead do:
# echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \
# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -

!echo "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \
curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -
!apt update

!apt-get install tensorflow-model-server

os.environ["MODEL_DIR"] = MODEL_DIR = "/gdrive/MyDrive/exported_model/"

# Commented out IPython magic to ensure Python compatibility.
# %%bash --bg
# nohup tensorflow_model_server \
#   --rest_api_port=8501 \
#   --model_name=exported_model\
#   --model_base_path="${MODEL_DIR}" >server.log 2>&1

!tail server.log

import tempfile
tempfile.gettempdir()

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# ls

import json
data = json.dumps({"signature_name": "serving_default", "instances": ["this is the best movie"]})

!pip install -q requests

import requests
headers = {"content-type": "application/json"}
json_response = requests.post('http://localhost:8501/v1/models/exported_model:predict', data=data, headers=headers)
predictions = json.loads(json_response.text)

import base64
import requests
import json
import tensorflow as tf

def _bytes_feature(value):
 return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def serialize_text(text):
 example = tf.train.Example(features=tf.train.Features(feature={
 'text': _bytes_feature(text)
 }))
 serialized_example = example.SerializeToString()
 return serialized_example

model_server_url = 'http://localhost:8501/v1/models/exported_model_latest:predict'

def predict(text):
 example = serialize_text(text)
 json_data = {
 "signature_name":"serving_default",
 "instances":[
 {
 "examples":{"b64": base64.b64encode(example).decode('utf-8')}
 }
 ]
 }

 resp = requests.post(model_server_url, json=json_data)
 return resp.json()

